{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"bp1HPTHGrVri"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11792,"status":"ok","timestamp":1686813372563,"user":{"displayName":"Jerome Tam","userId":"09544064474000675570"},"user_tz":-120},"id":"HjwEkz_pJ7Fs"},"outputs":[],"source":["import time\n","import os\n","import numpy as np\n","import tensorflow as tf\n","import scipy.io as sio\n","import pickle\n","import scipy.sparse as sp\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from jhyexp import my_KNN, my_Kmeans#, my_TSNE, my_Linear\n","from numpy import linalg as LA\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from models import GAT, HeteGAT, HeteGAT_multi\n","from utils import process"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"imW1eRAbrZim"},"source":["# Hyperparameteres"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686556303994,"user":{"displayName":"Jerome Tam","userId":"09544064474000675570"},"user_tz":-120},"id":"OyZ4hqv4pzYI"},"outputs":[],"source":["config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686556303994,"user":{"displayName":"Jerome Tam","userId":"09544064474000675570"},"user_tz":-120},"id":"70_8OrltqdY4","outputId":"f1869f49-9d73-41e8-f14c-ffdf6d4f677f"},"outputs":[],"source":["batch_size = 1\n","nb_epochs = 1\n","patience = 100\n","lr = 0.005  # learning rate\n","l2_coef = 0.001  # weight decay\n","# numbers of hidden units per each attention head in each layer\n","hid_units = [8]\n","n_heads = [8, 1]  # additional entry for the output layer\n","residual = False\n","nonlinearity = tf.nn.elu\n","model = HeteGAT_multi\n","\n","print('----- Opt. hyperparams -----')\n","print('lr: ' + str(lr))\n","print('l2_coef: ' + str(l2_coef))\n","print('----- Archi. hyperparams -----')\n","print('nb. layers: ' + str(len(hid_units)))\n","print('nb. units per layer: ' + str(hid_units))\n","print('nb. attention heads: ' + str(n_heads))\n","print('residual: ' + str(residual))\n","print('nonlinearity: ' + str(nonlinearity))\n","print('model: ' + str(model))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Aq-OYFJ0rgCs"},"source":["# Load Data and Mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1686556314580,"user":{"displayName":"Jerome Tam","userId":"09544064474000675570"},"user_tz":-120},"id":"N1RlKx04qqs1"},"outputs":[],"source":["def sample_mask(idx, l):\n","    \"\"\"Create mask.\"\"\"\n","    mask = np.zeros(l)\n","    mask[idx] = 1\n","    return np.array(mask, dtype=bool)\n","  \n","def load_data_dblp(path):\n","    \"\"\"\n","    Load the data \n","    here is where we difine the meta-paths\n","    \n","    \"\"\"\n","    with open(path, 'rb') as f:\n","      data = pickle.load(f)\n","    truelabels, truefeatures = data['labels'], data['features'].astype(float)\n","    N = truefeatures.shape[0]\n","    rownetworks = [np.maximum(data['PAP'] - np.eye(N), 0), np.maximum(data['PJP'] - np.eye(N), 0)]\n","\n","    y = truelabels\n","    train_idx = data['train_idx']\n","    val_idx = data['val_idx']\n","    test_idx = data['test_idx']\n","\n","    train_mask = sample_mask(train_idx, y.shape[0])\n","    val_mask = sample_mask(val_idx, y.shape[0])\n","    test_mask = sample_mask(test_idx, y.shape[0])\n","\n","    y_train = np.zeros(y.shape)\n","    y_val = np.zeros(y.shape)\n","    y_test = np.zeros(y.shape)\n","    y_train[train_mask, :] = y[train_mask, :]\n","    y_val[val_mask, :] = y[val_mask, :]\n","    y_test[test_mask, :] = y[test_mask, :]\n","\n","    truefeatures_list = [truefeatures, truefeatures]\n","    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MCQyiqaOrnoS"},"source":["# Path import\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def custom_sort_key(file_name):\n","    numeric_part = file_name.split(\"_\")[1]\n","    return int(numeric_part)\n","\n","def import_data(path,data_name):\n","    \"\"\"\n","    import name of file inside a folder\n","    path : The path to the folder wich cotains the Graph_t as file\n","    data_name : the name given to the data (ex:dblp,acm)\n","    \n","    \"\"\"\n","    L_path = []\n","    data_path = []\n","    file_names = sorted(os.listdir(path), key=custom_sort_key)\n","    i = 0\n","    for fname in file_names:\n","        dname = data_name + str(i)\n","        fpath = os.path.join(path, fname)\n","        if os.path.isfile(fpath):\n","            \n","            name = path + '\\\\' + fname\n","            L_path.append(name)\n","        data_path.append(dname)\n","        i += 1\n","    return L_path,data_path\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path = r\"C:\\Users\\JérômeTAM\\Desktop\\HAN\\DBLP\\DBLP GRAPH T=4\"\n","\n","\n","L_path,data_path = import_data(path,'dblp')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_path"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5EqKrttqsVby"},"source":["# Build Graph and HAN "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":212,"status":"ok","timestamp":1686556319446,"user":{"displayName":"Jerome Tam","userId":"09544064474000675570"},"user_tz":-120},"id":"WEXuGBx7rzzZ"},"outputs":[],"source":["def build_graph(model, fea_list,biases_list, y_train, y_val, y_test, train_mask, val_mask, test_mask,nb_nodes, ft_size,nb_classes,checkpt_file):\n","  with tf.Graph().as_default():\n","    with tf.name_scope('input'):\n","      ftr_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,\n","                                              shape=(batch_size, nb_nodes, ft_size),\n","                                              name='ftr_in_{}'.format(i))\n","                    for i in range(len(fea_list))]\n","\n","      bias_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,\n","                                                shape=(batch_size, nb_nodes, nb_nodes),\n","                                                name='bias_in_{}'.format(i))\n","                      for i in range(len(biases_list))]\n","\n","      lbl_in = tf.compat.v1.placeholder(dtype=tf.int32, \n","                                        shape=(batch_size, nb_nodes, nb_classes),\n","                                        name='lbl_in')\n","      \n","      msk_in = tf.compat.v1.placeholder(dtype=tf.int32, \n","                                        shape=(batch_size, nb_nodes),\n","                                        name='msk_in')\n","      \n","      attn_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n","      ffd_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n","      is_train = tf.compat.v1.placeholder(dtype=tf.bool, shape=(), name='is_train')\n","\n","    # forward\n","    logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,\n","                                                         attn_drop, ffd_drop,\n","                                                         bias_mat_list=bias_in_list,\n","                                                         hid_units=hid_units, n_heads=n_heads,\n","                                                         residual=residual, activation=nonlinearity)\n","    \n","    # cal masked_loss\n","    log_resh = tf.reshape(logits, [-1, nb_classes])\n","    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n","    msk_resh = tf.reshape(msk_in, [-1])\n","    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n","    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n","    # optimzie\n","    train_op = model.training(loss, lr, l2_coef)\n","\n","    saver = tf.compat.v1.train.Saver()\n","\n","    init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n","                       tf.compat.v1.local_variables_initializer())\n","\n","    vlss_mn = np.inf\n","    vacc_mx = 0.0\n","    curr_step = 0\n","\n","    with tf.compat.v1.Session(config=config) as sess:\n","      sess.run(init_op)\n","\n","      train_loss_avg = 0\n","      train_acc_avg = 0\n","      val_loss_avg = 0\n","      val_acc_avg = 0\n","\n","      for epoch in range(nb_epochs):\n","        tr_step = 0\n","\n","        tr_size = fea_list[0].shape[0]\n","\n","        # ================   training    ============\n","\n","        while tr_step * batch_size < tr_size:\n","          fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n","                 for i, d in zip(ftr_in_list, fea_list)}\n","\n","          fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n","                 for i, d in zip(bias_in_list, biases_list)}\n","\n","          fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n","                 msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n","                 is_train: True,\n","                 attn_drop: 0.6,\n","                 ffd_drop: 0.6}\n","\n","          fd = fd1\n","          fd.update(fd2)\n","          fd.update(fd3)\n","          _, loss_value_tr, acc_tr, att_val_train,train_final_embedding = sess.run([train_op, loss, accuracy, att_val,final_embedding],\n","                                                              feed_dict=fd)\n","          train_loss_avg += loss_value_tr\n","          train_acc_avg += acc_tr\n","          tr_step += 1\n","\n","\n","        vl_step = 0\n","        vl_size = fea_list[0].shape[0]\n","        \n","\n","                      # =============   val       =================\n","        while vl_step * batch_size < vl_size:\n","          fd1 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n","                for i, d in zip(ftr_in_list, fea_list)}\n","\n","\n","          fd2 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n","                for i, d in zip(bias_in_list, biases_list)}\n","\n","          fd3 = {lbl_in: y_val[vl_step * batch_size:(vl_step + 1) * batch_size],\n","                msk_in: val_mask[vl_step * batch_size:(vl_step + 1) * batch_size],\n","                is_train: False,\n","                attn_drop: 0.6,#0.0\n","                ffd_drop: 0.6}#0.0\n","          \n","          fd = fd1\n","          fd.update(fd2)\n","          fd.update(fd3)\n","          loss_value_vl, acc_vl,val_final_embedding = sess.run([loss, accuracy,final_embedding],\n","                                                 feed_dict=fd)\n","          val_loss_avg += loss_value_vl\n","          val_acc_avg += acc_vl\n","          vl_step += 1\n","            \n","                \n","                \n","\n","        print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n","        print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n","                  (train_loss_avg / tr_step, train_acc_avg / tr_step,\n","                   val_loss_avg / vl_step, val_acc_avg / vl_step))\n","\n","      \n","\n","        train_loss_avg = 0\n","        train_acc_avg = 0\n","        val_loss_avg = 0\n","        val_acc_avg = 0\n","      saver.save(sess, checkpt_file)\n","      saver.restore(sess, checkpt_file)\n","      print('load model from : {}'.format(checkpt_file))\n","      ts_size = fea_list[0].shape[0]\n","      ts_step = 0\n","      ts_loss = 0.0\n","      ts_acc = 0.0\n","      while ts_step * batch_size < ts_size:\n","        fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n","                   for i, d in zip(ftr_in_list, fea_list)}\n","        fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n","                   for i, d in zip(bias_in_list, biases_list)}\n","        fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n","                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n","            \n","                   is_train: False,\n","                   attn_drop: 0.6,\n","                   ffd_drop: 0.6}\n","\n","        fd = fd1\n","        fd.update(fd2)\n","        fd.update(fd3)\n","        loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n","                                                              feed_dict=fd)\n","        ts_loss += loss_value_ts\n","        ts_acc += acc_ts\n","        ts_step += 1\n","        \n","      \n","      sess.close()\n","      xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n","      yy = y_test[test_mask]\n","  \n","  return jhy_final_embedding,xx,yy,test_mask,train_final_embedding,val_final_embedding\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686556319714,"user":{"displayName":"Jerome Tam","userId":"09544064474000675570"},"user_tz":-120},"id":"buEHdDB5ZyEE"},"outputs":[],"source":["def compile_all(data_path,L_path):\n","  Lx = []\n","  Ly = []\n","  test_emb = []\n","  Lt = []\n","  val_emb = []\n","  train_emb = []\n","  for dataset,path in zip(data_path,L_path):\n","    featype = 'fea'\n","    \"\"\"\n","    replace r'C:\\Users\\JérômeTAM\\Desktop\\HAN\\CHECKPOINT by path \n","    \"\"\"\n","    checkpt_file = r'C:\\Users\\JérômeTAM\\Desktop\\HAN\\CHECKPOINT/{}/{}_allMP_multi_{}_.ckpt'.format(dataset, dataset, featype)\n","   \n","\n","    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(path)\n","    if featype == 'adj':\n","      fea_list = adj_list\n","    nb_nodes = fea_list[0].shape[0]\n","    ft_size = fea_list[0].shape[1]\n","    nb_classes = y_train.shape[1]\n","\n","    fea_list = [fea[np.newaxis] for fea in fea_list]\n","    adj_list = [adj[np.newaxis] for adj in adj_list]\n","    y_train = y_train[np.newaxis]\n","    y_val = y_val[np.newaxis]\n","    y_test = y_test[np.newaxis]\n","    train_mask = train_mask[np.newaxis]\n","    val_mask = val_mask[np.newaxis]\n","    test_mask = test_mask[np.newaxis]\n","\n","    biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n","\n","    emb,xx,yy,ts,v_emb,t_emb = build_graph(model, fea_list,biases_list, y_train, y_val, y_test, train_mask, val_mask, test_mask,nb_nodes, ft_size,nb_classes,checkpt_file)\n","\n","    Lx.append(xx)\n","    Ly.append(yy)\n","    test_emb.append(emb)\n","    Lt.append(ts)\n","    val_emb.append(v_emb)\n","    train_emb.append(t_emb)\n","  return Lx,Ly,test_emb,Lt,val_emb,train_emb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\"\n","xx : the list of nodes embedded for each Graph_t after HAN from the test set\n","\n","yy : the list of true labels for test-set\n","\n","test_mask : mask for test-set to use after we have the time aggregation\n","\n","train_emb : train embedding for each Graph_t after HAN \n","\n","val_emb : validation embedding for each Graph_t after HAN \n","\n","test_emb : test embedding for each Graph_t after HAN \n","\n","\n","\"\"\"\n","xx,yy,test_emb,test_mask,val_emb,train_emb = compile_all(data_path,L_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5SREyfu5bLMV"},"source":["# Time Aggregation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MyModel(tf.keras.Model):\n","    def __init__(self,nb_graph):\n","        super(MyModel, self).__init__()\n","        \"\"\"\n","        Model with conv layer as deep neural network, Maybe can be optimize with another deep neural network or a custom deep neural network\n","\n","        Chose between 1 conv for all Graph_t or x number of conv for x Graph_t\n","\n","        Forward defines the operations\n","\n","        Can be optimize with custom weight definition\n","\n","        inputs : a list of Graph_i for i not equal to t \n","        output : the predict Graph_t  \n","\n","        \n","        \"\"\"\n","        self.conv_layers1 = [tf.keras.layers.Conv1D(filters=64, kernel_size=1,activation=None,kernel_regularizer=tf.keras.regularizers.l1(0.1)) for i in range (nb_graph)]\n","        #self.conv_layers1 = tf.keras.layers.Conv1D(filters=64, kernel_size=1,activation='relu') \n","\n","    def forward(self, inputs):\n","        M = []\n","        G = []\n","        for i in range(len(inputs)):\n","            #x = self.conv_layers1(inputs[i])\n","            x = self.conv_layers1[i](inputs[i])\n","            #x = tf.reduce_sum(x, axis=0)\n","            #x = tf.nn.leaky_relu(x)\n","            x = tf.nn.softmax(tf.nn.leaky_relu(x))\n","            #x = tf.nn.softmax(x)\n","            #x = tf.reduce_sum(x, axis=0)\n","            M.append(x)\n","            \n","        for i in range(len(inputs)):\n","            G.append(M[i]*inputs[i])\n","        result = tf.reduce_sum(G, axis=0)\n","\n","        return result\n","\n","    def train(self, train_data, optimizer, loss):\n","        \"\"\"\n","        Perform the backprobagation to learn the weight of the neural network\n","        \"\"\"\n","        with tf.GradientTape() as tape:\n","            pred = self.forward(train_data[0])\n","            loss_value = loss(train_data[1], pred)\n","        grads = tape.gradient(loss_value, self.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n","        return loss_value\n","\n","    def evaluate(self, val_data, loss):\n","        \"\"\"\n","        \n","        \"\"\"\n","        pred = self.forward(val_data[0])\n","        loss_value = loss(val_data[1], pred)\n","        return loss_value\n","    \n","    def predict(self,pred_data):\n","        \"\"\"\n","        \"\"\"\n","        pred = self.forward(pred_data)\n","        return pred\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Adding 1 dim to get shape (1,nb_nodes,agg_dim)\n","\n","nb_nodes: number of nodes\n","agg_dim: dimension of aggregation from HAN \n","\n","\"\"\"\n","A_test = [test_emb[i][np.newaxis] for i in range(len(test_emb))]\n","A_train = [train_emb[i][np.newaxis] for i in range(len(train_emb))]\n","A_val = [val_emb[i][np.newaxis] for i in range(len(val_emb))]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nb_graph = len(A_train[0:-1])\n","model_time = MyModel(nb_graph)\n","optimizer = tf.keras.optimizers.Adam() \n","loss = tf.keras.losses.MeanSquaredError()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(100):\n","    total_loss = 0.0\n","    train_loss = model_time.train([A_train[0:-1], A_train[-1]], optimizer, loss)\n","    total_loss += train_loss\n","    \n","    val_loss = model_time.evaluate([A_val[0:-1], A_val[-1]], loss)\n","    print(f\"Epoch {epoch+1}: Train Loss = {total_loss}, Val Loss = {val_loss}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred = model_time.predict(A_test[0:-1])\n","xx_t = pred.numpy()[test_mask[-1]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Precision from HAN model \n","my_KNN(xx[-1],yy[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Precision from aggragated time \n","my_KNN(xx_t,yy[-1])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Prediction for graph5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Test with graph_5\n","\"\"\"\n","A_test_4 = A_test[0:4]\n","A_train_4 = A_train[0:4]\n","A_val_4 = A_val[0:4]\n","for i in range (5,len(A_test)):\n","    A_test_4.append(A_test[i])\n","    A_train_4.append(A_train[i])\n","    A_val_4.append(A_val[i])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nb_graph = len(A_train_4)\n","model_time = MyModel(nb_graph)\n","optimizer = tf.keras.optimizers.Adam() \n","loss = tf.keras.losses.MeanSquaredError()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(100):\n","    total_loss = 0.0\n","    train_loss = model_time.train([A_train_4, A_train[4]], optimizer, loss)\n","    total_loss += train_loss\n","    \n","    val_loss = model_time.evaluate([A_val_4, A_val[4]], loss)\n","    print(f\"Epoch {epoch+1}: Train Loss = {total_loss}, Val Loss = {val_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred = model_time.predict(A_test_4)\n","xx_t = pred.numpy()[test_mask[4]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#\n","my_KNN(xx[-1],yy[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#\n","my_KNN(xx_t,yy[-1])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["yFRvzAbBrP4r","bp1HPTHGrVri","imW1eRAbrZim","Aq-OYFJ0rgCs","MCQyiqaOrnoS","5EqKrttqsVby","5SREyfu5bLMV","fci9-ACutEUG"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3.9.16 ('My_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"cd0def18565d65ebaa1f51e800c0f0fdf338992f728844753024c2309df083c0"}}},"nbformat":4,"nbformat_minor":0}
